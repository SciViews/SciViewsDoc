# Introduction

R [@R] is a "language and environment for statistical computing". It is inspired from the S language and represents its Open Source counterpart that is now widely used (S is not much used yet). A big part of its success is a wide community of users and contributors. Hence, CRAN <https://cran.r-project.org> distributes more than 20,000 additional R packages (installable items that provide additional features to R).

As rich as it is, the R ecosystem is not the easiest one for a beginner. Python is often considered to be easier and more coherent. Yet, R was designed from the ground up to focus on data analysis, by limiting the exposition to technical aspect of computing. For instance, there are only two types of numeric values in base R: one integer (technically, Int32) and one floating point numeric value (Float64) where many computing languages expose ten or more different representations of numbers. However, to maintain its rich ecosystem, base R is very conservative. So, there are various aspects that makes it look old. Also, freedom is the rule. You can basically design functions in R the way you like. But this freedom means that it is more difficult for the users to master the different paradigms that are implemented.

There are often several different way to do something in R and there is even no convention on the name of functions. Hence, you find camelCase, snake_case or dot.case names and it is often hard to remind which convention was used for which function (also, the dot has a special meaning in R for S3 objects that does not play well with the dot.case, but that is another story). Another example of this diversity is the plot engines. There is one in base R, mainly using the `plot()` function. A second one, {lattice}, offered an alternative later on. A third engine is {ggplot2} [@ggplot2] that is now widely used. Yet the three systems coexist and useRs should know enough of all three to, at least, understand how to manipulate plots they produce.

So, would it be possible to cook a dialect of R that is as close as possible to base R, but that makes more homogeneous naming of function, more coherent user interface and that mixes base R with some of the best ideas you can find on CRAN? We believe so, and the SciViews::R [@SciViewsR] dialect is an attempt to do so. It is available in the {{< var sciviewsbox >}}.

> *Programs should be written for people to read, and only incidentally for machines to execute.*
>
> ***-- Abelson and Sussman** Structure and Interpretation of Computer Programs*

## Paradigms for data analysis

Data are present anywhere in a more or less exploitable form nowadays because our life is dominated by computers, smartphones and tablets that generate them constantly. Almost all professions are exposed to data. Data manipulation and analysis is a much needed skill for almost everyone.

They are good and bad ways to deal with data. We will quickly scan here the different paradigms that were developed to work with data, and we will determine which one are better, leading to the conclusion that R, and more particularly, SciViews::R represent a viable paradigm.

### The spreadsheet

Data are larger and larger. Statistics need replication. This leads inevitably to repetitive calculations on potentially a very large batch of data. Nobody analyse such data manually any more, and a dedicated software is needed. Perhaps one of the oldest, and still most used paradigm to work with data is the **spreadsheet**. The basic idea is to use a two-dimensional arrangement of cells in columns (named with letters) and rows (named with numbers). The A1 cell is the top left one. Cell B3 is on the second column and third row. Cells can contain different things: numbers, text, and formulas. A formula is a calculation expressed in a language that is specific to the spreadsheet. It is computed immediately upon display of the spreadsheet and the formula is replaced by the result in the visual presentation of the spreadsheet.

Here is a simple example. To add two numbers in a spreadsheet, you could write the first one in, say cell A1, the second one in cell A2. In cell A3 you could write a formula that will perform the calculation: something like `=SUM(A1,A2)`. You immediately see the result of your calculation displayed in cell A3. Moving the cursor to that cell reveals the formula that exists behind that result in the editing area on top of the spreadsheet.

![Two numbers and a formula that calculates their sum in Excel.](figures/01intro/excel_example.png)

One famous example of a spreadsheet software is [Microsoft Excel](https://www.microsoft.com/en-us/microsoft-365/excel). There are several others, for instance, [Numbers](https://www.icloud.com/numbers) on macOS, [LibreOffice Calc](https://www.libreoffice.org/discover/calc/), or [Google Sheets](https://workspace.google.com/products/sheets/). These spreadsheets are widely (overly?) used, in particular Microsoft Excel that is part of the Microsoft Office offering. This paradigm appears very powerful once one understands its logic. We may easily design increasingly complex spreadsheets that make all kinds of calculations on data. It is even possible to add plots directly inside the spreadsheet. Results and plots may then be very conveniently copy-pasted in say, Microsoft Word to write a report, or Microsoft PowerPoint to prepare a presentation.

As appealing this paradigm may be, there is a large literature that warns against it for anything else than very simple calculations on a tiny batch of data (TODO: citations needed here). The main problem is that calculations are hidden beneath results and the order in which all the formulas in the spreadsheet are computed obeys to a rather complicated algorithm that makes the understanding of what happens difficult, even for a modest spreadsheet. This is consequently prone to error, and indeed, several studies demonstrate that a large part of actual spreadsheets used in enterprises contain errors (TODO: citations required).

::: callout-caution
Refrain using a spreadsheet software like Excel to analyse your data. It is demonstrated that its paradigm is error-prone and calculations are difficult to understand and to debug.
:::

Another flaw in the workflow in an office suite is the decoupling of the computation (in the spreadsheet) and the display of results in the final document (report written in, say Microsoft Word, or presentation in Microsoft PowerPoint). **Every time you copy-paste some material from one software to another one, the workflow is disrupted because the origin of the material is lost.** If you need to update a plot, you must copy-paste it again from the updated spreadsheet. **Reproducible data analysis** is a key concept: all the steps from raw data to final figures and tables in a publication or a report must be fully replay-able automatically. Every time you copy-paste something, you disrupt the workflow and your analysis is not reproducible any more.

::: callout-caution
Do not rely on a data analysis workflow where you copy-paste material from one software to the other: your analysis is not reproducible that way.
:::

Also, data analysis is rarely done in one step. One tries different scenarios, compare them, refine them, to finally choose the best one. This results in several versions of the analysis. A versioning system, like [git](https://git-scm.com), is a much needed tool to keep track of the history of various versions of a document. Unfortunately, Microsoft Office documents as well as, most other spreadsheet, word processing, or slideshow software store their documents in a binary format that is not compatible with git. Versioning system are made for plain text. Spreadsheets are *not* plain text.

Finally, one could consider to keep its favorite spreadsheet software for simple tasks like data encoding of simple calculations. But it is still a bad idea. **Microsoft Excel or similar software tend to "interpret" your entries "semi-intelligently"**. Type something like `10/3` in Excel and it automatically converts it as the date March 3 of this year. You simply cannot rely on a software that does such silly changes behind your back for data encoding. Also, we have already seen that the Excel format .xls or .xlsx are binary format not very suited for versioning systems. It is far better to use text format like CSV for your data. A good example of a CSV editor that is suited to encode your data sets is [Modern CSV](https://www.moderncsv.com). Make you a favor: use CSV format and CSV editors instead of spreadsheet software for your raw data! Also keeping Excel for simple calculations refrain you to switch to a more suited software like R... and when is your calculation complex enough to switch? Will you master the other software for complex tasks if you do not first start using it for simple ones? Probably not.

::: callout-important
No matter the complexity of your calculations, do not do them in Excel, always use a proper statistical software for the analysis and encode your data in CSV using a proper CSV editor.
:::

### The Graphical User Interface

...

### The notebook

The notebook is another interesting paradigm.
